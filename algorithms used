sklearn is the python library that provides us with the supervised learning algorithms:

1) Logistic regression: Uses the sigmoid function on the dataset to tune the parameters for each class 
and for a new feature vector evaluates dot product of parameter vector and the feature vector.
Class that gets highest product is allotted to the vector.

2) SVM(support vector machine): Takes in the dataset and with the help of kernel function transforms it into one,
which can be segregated by appropriate hyperplane. 
New feature vector is assigned to the class according to its location relative to the hyperplane.Used when the decision boundary(hyperplane)
is non linear . 

3) KNN(K nearest neighbours) : Given a new feature vector, this algorithm finds k nearest feature vectors with respect to it 
  and the vector is allotted to the class which is present maximum number of times.
  The main disadvantage of this approach is that the algorithm must compute the distance and sort all the training data at each prediction,
  which can be slow if there are a large number of training examples.Choosing optimal K is also a challenge.
  A drawback of the basic "majority voting" classification occurs when the class distribution is skewed.
  That is, examples of a more frequent class tend to dominate the prediction of the new example,
  because they tend to be common among the k nearest neighbors due to their large number.
  optimal K is K = sqrt(m)/2, where m stands for the number of samples in your training dataset.K should be odd. 

4) Gaussian Naive Bayes (NB) : This assumes that all the variables are conditionally (within a class) independent 
and are normally distributed. So it calculates the mean and variance of each variable for every class.
  Using these and some basic probability it evaluates the probabilty the new feature vector to lie in each class. 
  Class with highest probability is allotted to the vector.

5) Classification and Regression Trees (CART) : This splits each feature vector(node) into subnodes 
so as to maintain common traits within them until met by a stopping condition.all possible splits are tried at every level,
the one that gives the highest decrease in cost function is considered.For regression-> sum squared function.
For classification -> gini index. Followed by pruning, to avoid overftting.

6) Linear Discriminant Analysis (LDA) : In a way similar to PCA (as both of them reduce the dimensions of the data),
but it focuses on minimizing the scatter and maximizing the separation between the classes.
It creates n-1 (where n is the number of classes) axes dividing the data into various classes.
For multiple classes first finds a central point then calculates distance from center of each class.	
